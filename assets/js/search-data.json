{
  
    
        "post0": {
            "title": "Visualizing Crime Incident Reports in Boston",
            "content": "Introduction . Moving to a new city in the US is a big deal. The rent is high. It often makes financial sense to move in with more people. This also means that one has to be considerate of other people&#39;s preferences and opinions when looking for apartments. At 9pm on a Monday, my flatmate informed me that he was having trouble convincing another flatmate that a certain neighbourhood in Boston was safe or at least as safe as where we currently live. This sounded like a nice Toy Problem to me. Something that I could use Data Visualization to help me with. After all, I had taken CS6250. I wasn&#39;t going to let Prof. Michelle Borkin down. . This exploration did not start with me looking for other implementations to place my work against but as an innocent exploration of the medium that is code and data. As I worked on the project, I also looked up if others had attempted to set sail on a similar voyage and of course, people had, many of them. A few of the projects are highlighted below. . This blogpost by Riley Huang was a wonderful read. Riley does a brilliant job of using a dataset that they are passionate about and coming up with a story and designing a learning plan around it. I&#39;m glad I found this blogpost before I posted this one. Riley uses Python and Pandas. . | This blogpost by Brian Ford of Wentworth Institute of Technology is another great read. Brian uses the data to make a web application using the Google Maps API. Brian uses Web Technologies to make the application. . | This blogpost by Namas Bhandari is a short jupyter notebook that performs a brief exploration of the data. . | This tableau dashboard by Amulya Aankul of Northeastern University is another great read. Amulya uses the data to make an effective tableau dashboard with slides to guide the reader through. Amulya also attempts to infer causal factors that could affect crime. Amulya uses datasets in addition to the Boston Crime dataset to build a narrative. . | . Over the course of the project, I decided to also explore ways to grouping the data as we will see soon. . Collecting the Data . The City of Boston as part of the Open Data Project publishes civic data on the Analyze Boston website. We are interested in Crime Incident Reports. . . The data for each year from 2015 is available as a separate csv file. There are two additional .xlsx files for the offense codes and the different field descriptions. Let us begin by scraping the download links off of the website. The download links are parents to the &lt;i&gt; tags that have the class icon-download. . Let&#39;s begin by importing the necessary libraries. Click on show to see the imports. . # importing libraries # for web scraping from bs4 import BeautifulSoup import requests as r # for data wrangling import pandas as pd from pandas import DateOffset import json import uuid import datetime import numpy as np from tslearn.barycenters import dtw_barycenter_averaging from tslearn.clustering import TimeSeriesKMeans from minisom import MiniSom # for plotting import networkx as nx import matplotlib.pyplot as plt import seaborn as sns from PIL import Image import io import pydot import matplotlib.colors as mcolors import random import math import matplotlib.dates as mdates # for map plotting import folium from folium.plugins import HeatMap, FastMarkerCluster, HeatMapWithTime from IPython.display import IFrame, display, Markdown, Latex, SVG, HTML import branca.colormap from IPython.display import Image as imgIpython # misc from tqdm import tqdm import os from collections import defaultdict . . We begin by getting the html source for the page. This can be achived using a simple GET request to the URL. . url = &quot;https://data.boston.gov/dataset/crime-incident-reports-august-2015-to-date-source-new-system&quot; res = r.get(url) . Next we use the BeautifulSoup library to parse the html source. Using the parsed output we select the csv and xlsx download links and download them. . soup = BeautifulSoup(res.text, &quot;html.parser&quot;) downloadBtns = soup.find_all(&#39;i&#39;, {&quot;class&quot;: &quot;icon-download&quot;}) urls = [ btn.parent[&#39;href&#39;] for btn in downloadBtns ] csvLinks = [ url for url in urls if url.endswith(&#39;.csv&#39;) ] xlLinks = [ url for url in urls if url.endswith(&#39;.xlsx&#39;) ] from tqdm import tqdm dfs = [] os.makedirs(&quot;./data&quot;, exist_ok=True) for link in tqdm(csvLinks): # downloading using requests # and writing to a file res = r.get(link) with open(&quot;./data/&quot; + link.split(&#39;/&#39;)[-1],&#39;w&#39;) as f: f.write(res.text) df = pd.read_csv(&quot;./data/&quot; + link.split(&#39;/&#39;)[-1], low_memory=False) dfs.append(df) xl_dfs = [] for link in tqdm(xlLinks): # downloading using requests # and writing to a file res = r.get(link) with open(&quot;./data/&quot; + link.split(&#39;/&#39;)[-1],&#39;wb&#39;) as f: f.write(res.content) df_xl = pd.read_excel(&quot;./data/&quot; + link.split(&#39;/&#39;)[-1]) xl_dfs.append(df) . 100%|███████████████████████████████████████████████| 8/8 [01:43&lt;00:00, 12.89s/it] . Looking at the Data . We then concat the dataframes into a single dataframe and take a look at it. . combined = pd.concat(dfs) combined.to_csv(&quot;./data/combined.csv&quot;) . Let&#39;s take a look at the datatypes of the columns. . combined.dtypes . . INCIDENT_NUMBER object OFFENSE_CODE int64 OFFENSE_CODE_GROUP object OFFENSE_DESCRIPTION object DISTRICT object REPORTING_AREA object SHOOTING object OCCURRED_ON_DATE object YEAR int64 MONTH int64 DAY_OF_WEEK object HOUR int64 UCR_PART object STREET object Lat float64 Long float64 Location object dtype: object . Let&#39;s fix the data types for the columns and also fill in NA values. The Offense codes are four digit codes. We will use the zfill function to fill in the missing digits with 0s. . combined_typed = combined.astype( { &quot;INCIDENT_NUMBER&quot;: &quot;str&quot;, &quot;OFFENSE_CODE&quot;: &quot;str&quot;, &quot;OFFENSE_CODE_GROUP&quot;: &quot;str&quot;, &quot;OFFENSE_DESCRIPTION&quot;: &quot;str&quot;, &quot;DISTRICT&quot;: &quot;str&quot;, &quot;REPORTING_AREA&quot;: &quot;str&quot;, &quot;SHOOTING&quot;: &quot;bool&quot;, &quot;OCCURRED_ON_DATE&quot;: &quot;datetime64&quot;, &quot;DAY_OF_WEEK&quot;: &quot;str&quot;, &quot;UCR_PART&quot;: &quot;str&quot;, &quot;STREET&quot;: &quot;str&quot;, &quot;Location&quot;: &quot;str&quot;, } ) combined_typed.fillna(&quot;&quot;, inplace=True) combined_typed[&quot;OFFENSE_CODE&quot;] = combined_typed[&quot;OFFENSE_CODE&quot;].map(lambda x: x.zfill(4)) # drop duplicates combined_typed.drop_duplicates(inplace=True) combined_typed.to_csv(&quot;./data/combined_typed.csv&quot;,index=False) combined_typed.head() . INCIDENT_NUMBER OFFENSE_CODE OFFENSE_CODE_GROUP OFFENSE_DESCRIPTION DISTRICT REPORTING_AREA SHOOTING OCCURRED_ON_DATE YEAR MONTH DAY_OF_WEEK HOUR UCR_PART STREET Lat Long Location . 0 S87066666 | 3301 | nan | VERBAL DISPUTE | B2 | 300 | False | 2022-04-07 19:30:00 | 2022 | 4 | Thursday | 19 | nan | THORNTON PLACE | 0.0 | 0.0 | (0, 0) | . 1 225520077 | 3115 | nan | INVESTIGATE PERSON | D14 | 786 | False | 2022-02-02 00:00:00 | 2022 | 2 | Wednesday | 0 | nan | WASHINGTON ST | 42.343081 | -71.141723 | (42.34308127134165, -71.14172267328729) | . 2 222934485 | 3831 | nan | M/V - LEAVING SCENE - PROPERTY DAMAGE | B2 | 319 | False | 2022-05-14 20:50:00 | 2022 | 5 | Saturday | 20 | nan | BLUE HILL AVE | 42.308038 | -71.083592 | (42.30803768051951, -71.08359213073199) | . 3 222924960 | 3301 | nan | VERBAL DISPUTE | C11 | 355 | False | 2022-04-09 16:30:00 | 2022 | 4 | Saturday | 16 | nan | GIBSON ST | 42.297555 | -71.059709 | (42.29755532959655, -71.05970910242573) | . 4 222648862 | 3831 | nan | M/V - LEAVING SCENE - PROPERTY DAMAGE | B2 | 288 | False | 2022-02-05 18:25:00 | 2022 | 2 | Saturday | 18 | nan | WASHINGTON ST | 42.329748 | -71.08454 | (42.329748204791635, -71.08454011649543) | . Let&#39;s look at the number of reports per year. . year_counts_df = pd.DataFrame( [[k,v] for k,v in combined_typed.groupby([&#39;YEAR&#39;])[&#39;YEAR&#39;].count().to_dict().items()], columns=[&#39;YEAR&#39;, &#39;COUNT&#39;] ) year_counts_df . YEAR COUNT . 0 2015 | 53404 | . 1 2016 | 99281 | . 2 2017 | 101198 | . 3 2018 | 98727 | . 4 2019 | 87184 | . 5 2020 | 70894 | . 6 2021 | 71721 | . 7 2022 | 38519 | . Oh right we can plot the values and take a look at them visually too. . ax = year_counts_df.plot(kind=&#39;bar&#39;, x=&#39;YEAR&#39;, y=&#39;COUNT&#39;) ax.set_title(&quot;Number of Reports Per Year&quot;) ax.set_xlabel(&quot;Year&quot;) ax.set_ylabel(&quot;Number of Reports&quot;) ax.legend().remove() . Let&#39;s now take a look at the number of reports per offense code for the entire dataset. . sns.set_context(&quot;notebook&quot;) offense_plot = combined_typed[&#39;OFFENSE_CODE&#39;].value_counts().plot(kind=&#39;bar&#39;,figsize=(40,6)) offense_plot.set_title(&quot;Number of Reports Per Offense Code&quot;) offense_plot.set_xlabel(&quot;Offense Code&quot;) offense_plot.set_ylabel(&quot;Number of Reports&quot;) offense_plot.legend().remove() . Ok That&#39;s a lot of Offense Codes. Making sense of this plot will need a really large monitor given how skewed the aspect ratio is. Let&#39;s first try to take a look at the 25 most common offense codes, their descriptions and their code groups to get a feel for the data. . top25_codes = combined_typed[[&#39;OFFENSE_CODE&#39;,&#39;OFFENSE_DESCRIPTION&#39;,&#39;OFFENSE_CODE_GROUP&#39;]] .value_counts() .head(25) .reset_index(name=&#39;COUNTS&#39;) top25_codes . OFFENSE_CODE OFFENSE_DESCRIPTION OFFENSE_CODE_GROUP COUNTS . 0 3006 | SICK/INJURED/MEDICAL - PERSON | Medical Assistance | 20968 | . 1 3115 | INVESTIGATE PERSON | Investigate Person | 20410 | . 2 3831 | M/V - LEAVING SCENE - PROPERTY DAMAGE | Motor Vehicle Accident Response | 17962 | . 3 1402 | VANDALISM | Vandalism | 16565 | . 4 0802 | ASSAULT SIMPLE - BATTERY | Simple Assault | 16338 | . 5 3301 | VERBAL DISPUTE | Verbal Disputes | 14469 | . 6 3410 | TOWED MOTOR VEHICLE | Towed | 12449 | . 7 3114 | INVESTIGATE PROPERTY | Investigate Property | 12358 | . 8 0617 | LARCENY THEFT FROM BUILDING | Larceny | 10075 | . 9 2647 | THREATS TO DO BODILY HARM | Other | 10058 | . 10 3201 | PROPERTY - LOST | Property Lost | 10047 | . 11 0614 | LARCENY THEFT FROM MV - NON-ACCESSORY | Larceny From Motor Vehicle | 9753 | . 12 3125 | WARRANT ARREST | Warrant Arrests | 9660 | . 13 0613 | LARCENY SHOPLIFTING | Larceny | 8813 | . 14 3802 | M/V ACCIDENT - PROPERTY  DAMAGE | Motor Vehicle Accident Response | 7318 | . 15 0619 | LARCENY ALL OTHERS | Larceny | 6715 | . 16 3803 | M/V ACCIDENT - PERSONAL INJURY | Motor Vehicle Accident Response | 5601 | . 17 0413 | ASSAULT - AGGRAVATED - BATTERY | Aggravated Assault | 5350 | . 18 3502 | MISSING PERSON - LOCATED | Missing Person Located | 5156 | . 19 1102 | FRAUD - FALSE PRETENSE / SCHEME | Fraud | 5044 | . 20 2629 | HARASSMENT | Harassment | 4530 | . 21 3207 | PROPERTY - FOUND | Property Found | 4073 | . 22 3501 | MISSING PERSON | Missing Person Reported | 3918 | . 23 0724 | AUTO THEFT | Auto Theft | 3908 | . 24 2610 | TRESPASSING | Other | 3541 | . Let us also take a look at the top 5 most common offense codes for each year. . df2015 = combined_typed[(combined_typed[&#39;YEAR&#39;] == 2015)] top5_codes_2015 = df2015[[&#39;OFFENSE_CODE&#39;,&#39;OFFENSE_DESCRIPTION&#39;]].value_counts().head(5).reset_index(name=&#39;COUNTS&#39;) display(Markdown(&#39;**Top 5 Codes in 2015:**&#39;)) top5_codes_2015 . . Top 5 Codes in 2015: . OFFENSE_CODE OFFENSE_DESCRIPTION COUNTS . 0 1402 | VANDALISM | 2659 | . 1 3831 | M/V - LEAVING SCENE - PROPERTY DAMAGE | 2651 | . 2 3115 | INVESTIGATE PERSON | 2511 | . 3 0802 | ASSAULT SIMPLE - BATTERY | 2393 | . 4 3006 | SICK/INJURED/MEDICAL - PERSON | 2384 | . df2016 = combined_typed[(combined_typed[&#39;YEAR&#39;] == 2016)] top5_codes_2016 = df2016[[&#39;OFFENSE_CODE&#39;,&#39;OFFENSE_DESCRIPTION&#39;]].value_counts().head(5).reset_index(name=&#39;COUNTS&#39;) display(Markdown(&quot;**Top 5 Codes in 2016:**&quot;)) top5_codes_2016 . . Top 5 Codes in 2016: . OFFENSE_CODE OFFENSE_DESCRIPTION COUNTS . 0 3115 | INVESTIGATE PERSON | 5775 | . 1 3006 | SICK/INJURED/MEDICAL - PERSON | 5495 | . 2 3831 | M/V - LEAVING SCENE - PROPERTY DAMAGE | 5071 | . 3 1402 | VANDALISM | 4954 | . 4 0802 | ASSAULT SIMPLE - BATTERY | 4471 | . df2017 = combined_typed[(combined_typed[&#39;YEAR&#39;] == 2017)] top5_codes_2017 = df2017[[&#39;OFFENSE_CODE&#39;,&#39;OFFENSE_DESCRIPTION&#39;]].value_counts().head(5).reset_index(name=&#39;COUNTS&#39;) display(Markdown(&quot;**Top 5 Codes in 2017:**&quot;)) top5_codes_2017 . . Top 5 Codes in 2017: . OFFENSE_CODE OFFENSE_DESCRIPTION COUNTS . 0 3115 | INVESTIGATE PERSON | 6668 | . 1 3006 | SICK/INJURED/MEDICAL - PERSON | 6278 | . 2 3831 | M/V - LEAVING SCENE - PROPERTY DAMAGE | 5221 | . 3 1402 | VANDALISM | 4773 | . 4 0802 | ASSAULT SIMPLE - BATTERY | 4636 | . df2018 = combined_typed[(combined_typed[&#39;YEAR&#39;] == 2018)] top5_codes_2018 = df2018[[&#39;OFFENSE_CODE&#39;,&#39;OFFENSE_DESCRIPTION&#39;]].value_counts().head(5).reset_index(name=&#39;COUNTS&#39;) display(Markdown(&quot;**Top 5 Codes in 2018:**&quot;)) top5_codes_2018 . . Top 5 Codes in 2018: . OFFENSE_CODE OFFENSE_DESCRIPTION COUNTS . 0 3006 | SICK/INJURED/MEDICAL - PERSON | 6811 | . 1 3115 | INVESTIGATE PERSON | 5460 | . 2 3831 | M/V - LEAVING SCENE - PROPERTY DAMAGE | 5019 | . 3 0802 | ASSAULT SIMPLE - BATTERY | 4838 | . 4 3301 | VERBAL DISPUTE | 4423 | . df2019 = combined_typed[(combined_typed[&#39;YEAR&#39;] == 2019)] top5_codes_2019 = df2019[[&#39;OFFENSE_CODE&#39;,&#39;OFFENSE_DESCRIPTION&#39;]].value_counts().head(5).reset_index(name=&#39;COUNTS&#39;) display(Markdown(&quot;**Top 5 Codes in 2019:**&quot;)) top5_codes_2019 . . Top 5 Codes in 2019: . OFFENSE_CODE OFFENSE_DESCRIPTION COUNTS . 0 3006 | SICK/INJURED/MEDICAL - PERSON | 5895 | . 1 3115 | INVESTIGATE PERSON | 5733 | . 2 3831 | M/V - LEAVING SCENE - PROPERTY DAMAGE | 4910 | . 3 0802 | ASSAULT SIMPLE - BATTERY | 3900 | . 4 3114 | INVESTIGATE PROPERTY | 3668 | . df2020 = combined_typed[(combined_typed[&#39;YEAR&#39;] == 2020)] top5_codes_2020 = df2020[[&#39;OFFENSE_CODE&#39;,&#39;OFFENSE_DESCRIPTION&#39;]].value_counts().head(5).reset_index(name=&#39;COUNTS&#39;) display(Markdown(&quot;**Top 5 Codes in 2020:**&quot;)) top5_codes_2020 . . Top 5 Codes in 2020: . OFFENSE_CODE OFFENSE_DESCRIPTION COUNTS . 0 3115 | INVESTIGATE PERSON | 5122 | . 1 3005 | SICK ASSIST | 4236 | . 2 3831 | M/V - LEAVING SCENE - PROPERTY DAMAGE | 3603 | . 3 1402 | VANDALISM | 3323 | . 4 3114 | INVESTIGATE PROPERTY | 3222 | . df2021 = combined_typed[(combined_typed[&#39;YEAR&#39;] == 2021)] top5_codes_2021 = df2021[[&#39;OFFENSE_CODE&#39;,&#39;OFFENSE_DESCRIPTION&#39;]].value_counts().head(5).reset_index(name=&#39;COUNTS&#39;) display(Markdown(&quot;**Top 5 Codes in 2021:**&quot;)) top5_codes_2021 . . Top 5 Codes in 2021: . OFFENSE_CODE OFFENSE_DESCRIPTION COUNTS . 0 3115 | INVESTIGATE PERSON | 6841 | . 1 3005 | SICK ASSIST | 4969 | . 2 3831 | M/V - LEAVING SCENE - PROPERTY DAMAGE | 4678 | . 3 3114 | INVESTIGATE PROPERTY | 3531 | . 4 1402 | VANDALISM | 3287 | . df2022 = combined_typed[(combined_typed[&#39;YEAR&#39;] == 2022)] top5_codes_2022 = df2022[[&#39;OFFENSE_CODE&#39;,&#39;OFFENSE_DESCRIPTION&#39;]].value_counts().head(5).reset_index(name=&#39;COUNTS&#39;) display(Markdown(&quot;**Top 5 Codes in 2022:**&quot;)) top5_codes_2022 . . Top 5 Codes in 2022: . OFFENSE_CODE OFFENSE_DESCRIPTION COUNTS . 0 3115 | INVESTIGATE PERSON | 4172 | . 1 1831 | SICK ASSIST | 2926 | . 2 3831 | M/V - LEAVING SCENE - PROPERTY DAMAGE | 2426 | . 3 3114 | INVESTIGATE PROPERTY | 1818 | . 4 3410 | TOWED MOTOR VEHICLE | 1669 | . Let&#39;s now take a look at a heatmap of the numbers of reports for the year 2021. We&#39;ll be using folium for map visualization. Let us also add markers that correspond to the agregated counts for a geographic region. . hmap = folium.Map(location=[42.336082675630315, -71.09991318861809], zoom_start=15,tiles=&#39;OpenStreetMap&#39;, ) data = list(zip(df2021.Lat.values, df2021.Long.values)) steps=20 colormap = branca.colormap.linear.YlOrRd_09.scale(0, 1).to_step(steps) gradient_map=defaultdict(dict) for i in range(steps): gradient_map[1/steps*i] = colormap.rgb_hex_str(1/steps*i) colormap.add_to(hmap) hm_wide = HeatMap( data, min_opacity=0.2, radius=17, blur=15, max_zoom=1, gradient=gradient_map, name=&#39;heatmap&#39; ) hmap.add_child(hm_wide) markers = FastMarkerCluster(data=data,name=&#39;markers&#39;) hmap.add_child(markers) folium.LayerControl().add_to(hmap) hmap.save(&quot;../images/heatmap.html&quot;) . IFrame(&quot;/images/heatmap.html&quot;, width=&quot;100%&quot;, height=&quot;500px&quot;) . . Keep in mind that this is the aggredgated-count heatmap for all incident types for 2021. To see the heatmap for a specific year we can change the line data = list(zip(df2021.Lat.values, df2021.Long.values)) to our desired year. In order to see the heatmap for a sepecific indident type we can add a filter to the dframe. For example, to see the heatmap for all crimes involving the offense code 3115 we can add a filter to the dataframe like so: df2021[df2021.OFFENSE_CODE == &#39;3115&#39;]. . We can also take a look at a heatmap over time. For this we will use the folium plugin HeatMapWithTime. The data we need to past is a list of lists of locations. The index for the data will be our bins of time. . combined_typed[&quot;OCCURRED_ON_DATE&quot;] = pd.to_datetime(combined_typed[&quot;OCCURRED_ON_DATE&quot;]) c_indexed = combined_typed.set_index(&#39;OCCURRED_ON_DATE&#39;) c_indexed_grouped = c_indexed.groupby([&#39;OCCURRED_ON_DATE&#39;]) . Let&#39;s also define a helper function to convert a latitude and longitude string of the form (42.3601, -71.0589) to a tuple of floats. . def convert_lat_long(lat_long_str): lat_long_str = lat_long_str.replace(&#39;(&#39;,&#39;&#39;) lat_long_str = lat_long_str.replace(&#39;)&#39;,&#39;&#39;) lat_long_str = lat_long_str.split(&#39;,&#39;) lat = float(lat_long_str[0]) long = float(lat_long_str[1]) return [lat,long] . Let&#39;s resample the data to get the number of reports per Month and create a list of lists of locations for each month. . sampled = c_indexed.resample(&quot;M&quot;) locations = sampled.agg(pd.Series.tolist)[&quot;Location&quot;] data = [ [ convert_lat_long(point) for point in timeframe] for timeframe in locations] # remove points that are undefined and (0,0) data = [point for point in data if point[0] != 0 and point[1] != 0] data = [point for point in data if point[0] is not None and point[1] is not None] . All together now. . hmap = folium.Map(location=[42.336082675630315, -71.09991318861809], zoom_start=15,tiles=&#39;OpenStreetMap&#39;, ) steps=20 colormap = branca.colormap.linear.YlOrRd_09.scale(0, 1).to_step(steps) gradient_map=defaultdict(dict) for i in range(steps): gradient_map[1/steps*i] = colormap.rgb_hex_str(1/steps*i) colormap.add_to(hmap) hm_wide = HeatMapWithTime( data, min_opacity=0.2, radius=17, max_opacity=0.8, gradient=json.dumps(gradient_map), ) hmap.add_child(hm_wide) hmap.save(&quot;../images/heatmap_time.html&quot;) . IFrame(&quot;/images/heatmap_time.html&quot;, width=&quot;100%&quot;, height=&quot;500px&quot;) . . It is obvious that maps are excellent for visualizing geographical data. Their limits as a visual aid are put to the test when we have multiple data class types like in the case of incident reports. Naively we could visualize different classes with different colors. We 254 different classes. Using all of them would make the visualization immpossible to interpret. Using the viz principle of Details on Demand, another strategy would be to use brushing and linking to bind a set of selections from a less granualar visualization like a bar chart or a dropdown list (a table) to a map. . The humble line chart is never one to disappoint. On that note, let us take a look at the number of counts per month for the time range as a time series line chart. . ax = c_indexed.resample(&quot;M&quot;).count()[&quot;INCIDENT_NUMBER&quot;].plot(figsize=(30,10)) ax.set_title(&quot;Number of Reports Per Month&quot;) ax.set_xlabel(&quot;Month&quot;) ax.set_ylabel(&quot;Number of Reports&quot;) display(Markdown(&quot;### Total Reports per Month&quot;)) ax.legend().remove() . Total Reports for Each Month . We can get a more fine grained view of the data by taking a look at the number of counts per week for the time range. . # resample weekly ax = c_indexed.resample(&quot;W&quot;).count()[&quot;INCIDENT_NUMBER&quot;].plot(figsize=(30,10)) ax.set_title(&quot;Number of Reports Per Week&quot;) ax.set_xlabel(&quot;Week&quot;) ax.set_ylabel(&quot;Number of Reports&quot;) display(Markdown(&quot;### Total Reports per Week&quot;)) ax.legend().remove() . . Total Reports for Each Week . The data appears to be seasonal in nature. Lets take a look at a specific Incident Code. . # select rows matching a specific offense code and resample weekly offense_code = &#39;0613&#39; time_scale = &quot;W&quot; ax = c_indexed[c_indexed[&quot;OFFENSE_CODE&quot;] == offense_code].resample(time_scale).count()[&quot;INCIDENT_NUMBER&quot;].plot(figsize=(30,10)) description = combined_typed[combined_typed[&quot;OFFENSE_CODE&quot;] == offense_code][&quot;OFFENSE_DESCRIPTION&quot;].iloc[0] ax.set_xlabel(f&quot;Year, data grouped by {time_scale}&quot;) ax.set_ylabel(&quot;Number of Reports&quot;) ax.set_title(f&quot;Number of Reports for Offense {offense_code} Per {time_scale}&quot;) display(Markdown(f&quot;### Reports for Code {offense_code} ({description}) Grouped by Week&quot;)) ax.legend().remove() . . Reports for Code 0613 (LARCENY SHOPLIFTING) Grouped by Week . Larceny seems to have a dip in the first half of 2020. The overall trend appears to be stable. . # select rows matching a specific offense code and resample weekly offense_code = &#39;1831&#39; time_scale = &quot;W&quot; ax = c_indexed[c_indexed[&quot;OFFENSE_CODE&quot;] == offense_code].resample(time_scale).count()[&quot;INCIDENT_NUMBER&quot;].plot(figsize=(30,10)) description = combined_typed[combined_typed[&quot;OFFENSE_CODE&quot;] == offense_code][&quot;OFFENSE_DESCRIPTION&quot;].iloc[0] ax.set_xlabel(f&quot;Year, data grouped by {time_scale}&quot;) ax.set_ylabel(&quot;Number of Reports&quot;) ax.set_title(f&quot;Number of Reports for Offense {offense_code} Per {time_scale}&quot;) display(Markdown(f&quot;### Reports for Code {offense_code} ({description}) Grouped by Week&quot;)) ax.legend().remove() . . Reports for Code 1831 (SICK ASSIST) Grouped by W . Interestingly the number of sick assists have shot up in 2022. One can hypothesise that this is a filing error. The empty space between the end of 2019 and 2022 is likely due to the fact that COVID-19 induced filing delays. . # select rows matching a specific offense description and resample weekly ax = c_indexed[c_indexed[&quot;OFFENSE_DESCRIPTION&quot;] == &#39;VERBAL DISPUTE&#39;].resample(&quot;W&quot;).count()[&quot;INCIDENT_NUMBER&quot;].plot(figsize=(30,10)) ax.set_xlabel(&quot;Year, grouped by Week&quot;) ax.set_ylabel(&quot;Number of Reports&quot;) ax.set_title(&quot;Number of Reports for VERBAL DISPUTE Per Week&quot;) display(Markdown(&quot;### Investigate Verbal Dispute Reports Grouped by Week&quot;)) ax.legend().remove() . . Investigate Verbal Dispute Reports Grouped by Week . The onset of the pandemic also seems to correlate with a drop in the number of verbal disputes. Masks seem to have worked :bowtie:! . # select rows matching a specific offense description and resample weekly ax = c_indexed[c_indexed[&quot;OFFENSE_DESCRIPTION&quot;] == &#39;INVESTIGATE PERSON&#39;].resample(&quot;W&quot;).count()[&quot;INCIDENT_NUMBER&quot;].plot(figsize=(30,10)) ax.set_xlabel(&quot;Year, grouped by Week&quot;) ax.set_ylabel(&quot;Number of Reports&quot;) ax.set_title(&quot;Number of Reports for INVESTIGATE PERSON Per Week&quot;) display(Markdown(&quot;### Investigate Person Reports Grouped by Week&quot;)) ax.legend().remove() . . Investigate Person Reports Grouped by Week . This looks interesting. There&#39;s some interesting end of the year peaking happening in the number of Investigate Person reports. There could be so many factors at play here. These demands futher inquiry. . # select rows that have Shooting set to true and resample daily ax = c_indexed[c_indexed[&quot;SHOOTING&quot;] == True].resample(&quot;D&quot;).count()[&quot;INCIDENT_NUMBER&quot;].plot(figsize=(30,10)) ax.set_xlabel(&quot;Year, grouped by Day&quot;) ax.set_ylabel(&quot;Number of Shooting Reports&quot;) ax.set_title(&quot;Number of Reports with shootings Per Day&quot;) display(Markdown(&quot;### Shootings Grouped by Day&quot;)) ax.legend().remove() . . Shootings Grouped by Day . Oh my! What everyone was waiting for. Shootings dropped after 2019? This is weird. Did they change the filing code? or Did shootings drop because of change in adminisitration? . That&#39;s a drastic dip in the number of shootings after 2019. Unfortunately I am not well equipped to answer why this is the case. Looking into it will be a future project. . Grouping Incidents I (Prefix Tree) . We can begin trying to think about how to group the incidents together given the sheer number of codes there are. It would be much easier to look at shared trends in the data if we could somehow arrange related incidents together. . A first method is based on prefix codes. The Incident codes are in what appears to be prefix code. Prefix codes are codes where items sharing a common prefix share a parent. . . Let&#39;s build the prefix code tree of the offense codes. We begin by defining a recursive function to build the prefix code tree. The function process takes in a list of codes and returns a dictionary of prefixes and their corresponding children. The process is applied to each child node till we reach the leaf nodes. . def process(items,n=0): if type(items) == list: groups = {} for item in items: s = str(item) if n &gt;= len(s): return items[0] if s[n] in groups: groups[s[n]].append(item) else: groups[s[n]] = [item] for group in groups: groups[group] = process(groups[group],n+1) return groups elif type(items) == dict: return {k:process(v,n+1) for k,v in items.items()} codeList = list(c_indexed[&quot;OFFENSE_CODE&quot;].unique()) # remove code 99999 codeList.remove(&quot;99999&quot;) . We add a key s to the dictionary as a root node. . tree = {&#39;s&#39; : process(codeList)} . As a result we can now query the dictionary for codes that begin with 10 as follows, . tree[&#39;s&#39;][&#39;1&#39;][&#39;0&#39;] . {&#39;0&#39;: {&#39;1&#39;: &#39;1001&#39;, &#39;0&#39;: &#39;1000&#39;, &#39;2&#39;: &#39;1002&#39;}} . As you can see each dictionary key is a prefix and the associated value is either the resulting code or a dictionary of prefixes and their children. . Let us rename the nodes by appending a unique code to each key. Since we have many repeating prefix keys it is possible that making a network of nodes will result in an incorrect vizualization since vertex names are unique in most graph/network processing packages like pydot and networkx. . sns.reset_orig() sns.set_style(&quot;white&quot;) rename_dict = {} def alter_keys(dictionary, func): empty = {} for k, v in dictionary.items(): n = func(k) if isinstance(v, dict): empty[n] = alter_keys(v, func) else: empty[n] = v return empty def append_id(name): new_name = name + &quot;_&quot; + str(uuid.uuid4()) rename_dict[name] = new_name return new_name renamed_tree = alter_keys(tree, append_id) . We now define a second recursive function that takes as input the renamed prefix tree and adds the edges to networkx digraph. We create an empty graph and the function addEdgeT adds edges to our empty graph. . G = nx.DiGraph() def visit(node, parent=None): for k,v in node.items(): if isinstance(v, dict): # We start with the root node whose parent is None # we don&#39;t want to graph the None node if parent: addEdgeT(parent, k) visit(v, k) else: addEdgeT(parent, k) # drawing the label using a distinct name addEdgeT(k, k+&#39;_&#39;+str(v)) def addEdgeT(parent_name, child_name): G.add_edge(parent_name, child_name) visit(renamed_tree) . We can now check if the graph is planar and if it is a tree. . print(f&quot; Is it planar ? : {nx.check_planarity(G)[0]}&quot;) . Is it planar ? : True . print(f&quot; Is it a tree ? : {nx.is_tree(G)}&quot;) . Is it a tree ? : True . Let&#39;s take a look at the network of prefixes. . nx.draw(G,node_size=2) . This hairball is not a canonical visualization of a tree. Let&#39;s use a different layout. A more pleasing visualization of a tree is a radial tree. . . This is available to us as part of networkx which includes sections of the pydot library and some graphviz layout algorithms. We are interested in the twopi layout of a tree. We drive the edge colors based on the prefix code of the leaf nodes. Optionally we drive the edge labels using the prefix code of node. We next drive the size of the vertices and the edge width based on the icident counts for each prefix code. . Here I use arebitray functions to map the values to edge widths and vertex sizes. If n is the number of incidents for a prefix code, The edge widths are mapped to math.log(n,200) and the vertex sizes are mapped to (n/1000)*5. . fig, ax = plt.subplots(figsize=(12,12)) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;bottom&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) # pos = nx.planar_layout(G) pos = nx.nx_pydot.graphviz_layout(gg, prog=&quot;twopi&quot;) c_x,c_y = pos[rename_dict[&#39;s&#39;]] # vertex colors colors = list(mcolors.TABLEAU_COLORS.keys()) used_colors = [] vertex_colors = [] vertex_color_dict = {} # color vertex if it is of type int white if it is not for v in gg.nodes: if type(v) == int: first_digits = str(v).zfill(4)[0:1] if first_digits not in vertex_color_dict: vertex_color_dict[first_digits] = random.choice(list(set(colors) - set(used_colors))) used_colors.append(vertex_color_dict[first_digits]) vertex_colors.append(vertex_color_dict[first_digits]) else: vertex_colors.append(vertex_color_dict[first_digits]) else: vertex_colors.append(&#39;black&#39;) # edge colors edge_colors = [ vertex_color_dict[str(v).zfill(4)[0:1]] if isinstance(v,int) else &#39;grey&#39; for u,v in gg.edges] # edge labels edge_labels = {} for e in gg.edges: if type(e[1]) == int: edge_labels[(e[0],e[1])] = str(e[1]).zfill(4) else: edge_labels[(e[0],e[1])] = e[1].split(&#39;_&#39;)[0] # vertex size counts = {} vertex_size = [] for v in tqdm(gg.nodes): if type(v) == int: code = str(v).zfill(4) # count the number of incidents with this code n = c_indexed[c_indexed[&quot;OFFENSE_CODE&quot;] == code].count()[&quot;INCIDENT_NUMBER&quot;] counts[v] = n # log scale vertex_size.append((n/1000)*5) else: vertex_size.append(0.01) # edge width edge_width = [math.log(counts[v],200) if isinstance(v,int) else 0.2 for u,v in gg.edges] nx.draw_networkx_edges(gg,pos,ax=ax,edge_color=edge_colors,width=edge_width,arrows=False) nx.draw_networkx_nodes(gg,pos=pos,node_size=vertex_size,node_color=vertex_colors,ax=ax) #nx.draw_networkx_labels(gg,labels=labb,pos=pos,font_size=5,ax=ax,font_color=&#39;white&#39;) nx.draw_networkx_edge_labels(gg,pos,edge_labels=edge_labels,ax=ax,font_size=3.5,font_color=&#39;black&#39;,label_pos=0.4) buf = io.BytesIO() fig.savefig(&#39;plots/categories_twopi_lab_color.svg&#39;) ax.set_title(&#39;Categories&#39;,fontsize=20) fig.savefig(buf) buf.seek(0) plt.close(fig) im = Image.open(buf) im.save(&#39;plots/categories_twopi_lab_color.png&#39;) # save pos as json with open(&#39;plots/pos_twopi_color.json&#39;, &#39;w&#39;) as fp: json.dump(pos, fp) . . 100%|██████████| 637/637 [00:09&lt;00:00, 64.53it/s] . Let&#39;s take a look at the resulting vizualization. . style = &quot;&lt;style&gt;svg{width:100% !important;height:100% !important;&lt;/style&gt;&quot; display(HTML(style)) SVG(&#39;plots/categories_twopi_lab_color.svg&#39;) . 2022-09-11T22:09:12.530288 image/svg+xml Matplotlib v3.5.2, https://matplotlib.org/ Here, each color is associated with codes that share the same prefix at a particular position. For example 0001 and 0000 share the same parent node corresponding to the prefix 000. The scales are arbitrary and thus need trial and error to make them work with the visualization. As you can see the circles are impossible to see for some incidents. Some incident counts are just too low to be visible. An alternative visualization could use the normalized incident counts to drive the size of the circles. . Time for some animations. Let us make the plot move with time. This visualization is by no means a replacement for the humble line plot but an aid to help the user demand the right plots to later visualize. We begin by sorting the dataframe by its index. Here the index is the column OCCURRED_ON_DATE. . We can now group by the OFFENSE_CODE column and count the number of occurrences per rolling window. We can use these counts to drive the plot and generate frames of the animation. Let us take a look at slice of the output. . c_indexed df = pd.crosstab(combined_typed[&quot;OCCURRED_ON_DATE&quot;].dt.to_period(&#39;2W&#39;), combined_typed[&#39;OFFENSE_CODE&#39;]) df . OFFENSE_CODE 0100 0111 0112 0121 0122 0123 0300 0301 0311 0315 ... 3803 3805 3807 3810 3811 3820 3821 3830 3831 99999 . OCCURRED_ON_DATE . 2015-06-15/2015-06-21 0 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 3 | 0 | ... | 20 | 9 | 11 | 1 | 1 | 4 | 1 | 8 | 73 | 0 | . 2015-06-22/2015-06-28 0 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 3 | 0 | ... | 31 | 2 | 5 | 7 | 6 | 6 | 0 | 10 | 97 | 0 | . 2015-06-29/2015-07-05 0 | 3 | 0 | 0 | 0 | 0 | 0 | 20 | 0 | 0 | ... | 30 | 5 | 3 | 2 | 6 | 7 | 1 | 5 | 72 | 0 | . 2015-07-06/2015-07-12 0 | 3 | 0 | 0 | 0 | 0 | 0 | 22 | 2 | 0 | ... | 23 | 10 | 4 | 9 | 6 | 11 | 4 | 6 | 90 | 0 | . 2015-07-13/2015-07-19 0 | 0 | 0 | 0 | 0 | 0 | 0 | 20 | 3 | 0 | ... | 33 | 1 | 1 | 12 | 7 | 7 | 1 | 10 | 80 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2022-06-13/2022-06-19 0 | 1 | 0 | 0 | 0 | 0 | 0 | 18 | 0 | 0 | ... | 16 | 6 | 4 | 8 | 4 | 13 | 0 | 6 | 94 | 0 | . 2022-06-20/2022-06-26 0 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 0 | 0 | ... | 10 | 5 | 6 | 9 | 1 | 12 | 1 | 5 | 76 | 0 | . 2022-06-27/2022-07-03 0 | 0 | 0 | 0 | 0 | 0 | 0 | 12 | 0 | 0 | ... | 19 | 8 | 2 | 5 | 1 | 5 | 1 | 3 | 100 | 0 | . 2022-07-04/2022-07-10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 0 | 0 | ... | 21 | 0 | 4 | 8 | 1 | 9 | 3 | 5 | 71 | 0 | . 2022-07-11/2022-07-17 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | ... | 6 | 1 | 4 | 0 | 1 | 5 | 0 | 2 | 28 | 0 | . 370 rows × 254 columns . We can now build a list of count dictionaries for each time period. We can then use these dictionaries to drive the plot using the same code. . i_s = df.index.to_series().values.tolist() count_list = [] for ix,i in tqdm(enumerate(i_s)): counts = {} d = df.iloc[ix,:].to_dict() for v in gg.nodes: if type(v) == int: code = str(v).zfill(4) counts[v] = d[code] count_list.append(counts) . 370it [00:00, 2751.95it/s] . frames = [] for counts in tqdm(count_list): vertex_size = [] for v in gg.nodes: if type(v) == int: code = str(v).zfill(4) n = counts[v] vertex_size.append(n) else: vertex_size.append(0.01) fig, ax = plt.subplots(figsize=(12,12)) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;bottom&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) # pos = nx.planar_layout(G) pos = nx.nx_pydot.graphviz_layout(gg, prog=&quot;twopi&quot;) c_x,c_y = pos[rename_dict[&#39;s&#39;]] edge_colors = [ vertex_color_dict[str(v).zfill(4)[0:1]] if isinstance(v,int) else &#39;grey&#39; for u,v in gg.edges] edge_width = [math.log(max(0.1,counts[v]),200) if isinstance(v,int) else 0.2 for u,v in gg.edges] nx.draw_networkx_edges(gg,pos,ax=ax,edge_color=edge_colors,width=edge_width,arrows=False) nx.draw_networkx_nodes(gg,pos=pos,node_size=vertex_size,node_color=vertex_colors,ax=ax) #nx.draw_networkx_labels(gg,labels=labb,pos=pos,font_size=5,ax=ax,font_color=&#39;white&#39;) nx.draw_networkx_edge_labels(gg,pos,edge_labels=edge_labels,ax=ax,font_size=3.5,font_color=&#39;black&#39;,label_pos=0.4) buf = io.BytesIO() fig.savefig(&#39;plots/categories_twopi_lab_color.svg&#39;) fig.savefig(buf) buf.seek(0) plt.close(fig) im = Image.open(buf) frames.append(im) gif_name = &#39;plots/categories_twopi_lab_color.gif&#39; frames[0].save(gif_name, format=&#39;GIF&#39;, append_images=frames[1:], save_all=True, duration=100, loop=0) . . 100%|██████████| 370/370 [25:27&lt;00:00, 4.13s/it] . Adding a timestamp to the plot should help us understand the plot better. We will use PIL to add the timestamp to the plot since generating the matplotlib frames again is a computationally expensive operation. . from PIL import ImageDraw new_frames = [] for i,frame in enumerate(frames): im = frames[i].copy() # add frame to image draw = ImageDraw.Draw(im) draw.text((130,130), str(i_s[i]), fill=(0,0,0)) new_frames.append(im) gif_name = &#39;plots/categories_twopi_lab_color.gif&#39; new_frames[0].save(gif_name, format=&#39;GIF&#39;, append_images=new_frames[1:], save_all=True, duration=100, loop=0) . . Grouping Incidents II (Time Series Clustering) . Anohter way to group this data is to first convert the incident counts to a min-max normalized (0-1) time series and clustering the different codes together based on Time Series CLustering methods. Two methods are descibed in this Kaggle notebook. We will be using K-means clustering from the package tslearn for this task. . We first convert the i_s index list to a datetime object list. This will help us plot the time series with appropriate x-axis labels. . yx = [datetime.datetime.fromtimestamp(int(i.to_timestamp().timestamp())) for i in i_s] . We now build a list of lists of each incident code. These lists are time series . count_series = {} for v in gg.nodes: if type(v) == int: code = str(v).zfill(4) count_series[v] = df[code].to_list() . count_normalized_series = {} for k,v in count_series.items(): vals = np.array(v) # count_normalized_series[k] = (vals - np.mean(vals))/np.std(vals) _min = np.min(vals) _max = np.max(vals) count_normalized_series[k] = (vals-_min)/(_max-_min) . data_index = list(count_normalized_series.keys()) data = np.array(list(count_normalized_series.values())) . Next we will try using TimeSeriesKMeans to cluster the time series. . cluster_count = math.ceil(math.sqrt(len(data))) # number of clusters km = TimeSeriesKMeans(n_clusters=cluster_count, metric=&quot;euclidean&quot;) labels = km.fit_predict(data) . plot_count = math.ceil(math.sqrt(cluster_count)) fig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25)) fig.suptitle(&#39;Clusters - Averaged&#39;,fontsize=60) row_i=0 column_j=0 # For each label there is, # plots every series with that label for label in set(labels): cluster = [] for i in range(len(labels)): if(labels[i]==label): axs[row_i, column_j].plot(yx,data[i],c=&quot;gray&quot;,alpha=0.4) cluster.append(data[i]) if len(cluster) &gt; 0: axs[row_i, column_j].plot(yx,np.average(np.vstack(cluster),axis=0),c=&quot;red&quot;) axs[row_i, column_j].set_title(&quot;Cluster &quot;+str(label)) locator = mdates.AutoDateLocator() formatter = mdates.ConciseDateFormatter(locator) axs[row_i, column_j].xaxis.set_major_locator(locator) axs[row_i, column_j].xaxis.set_major_formatter(formatter) column_j+=1 if column_j%plot_count == 0: row_i+=1 column_j=0 plt.show() . Here the red line is the average time series for the cluster. . How many incidents do we have per cluster? . pdata = [] for label in set(labels): cluster = [] for i in range(len(labels)): if(labels[i]==label): cluster.append(data[i]) pdata.append((label,len(cluster))) print(&quot;Cluster : &quot; + str(label) + &quot; : &quot; + str(len(cluster))) # plot pdata fig, ax = plt.subplots(figsize=(10,5)) ax.bar([str(x[0]) for x in pdata],[x[1] for x in pdata]) ax.set_title(&quot;Number of Incidents in Each Cluster&quot;) ax.set_xlabel(&quot;Cluster&quot;) ax.set_ylabel(&quot;Number of Incident Types&quot;) plt.show() . Cluster : 0 : 5 Cluster : 1 : 98 Cluster : 2 : 23 Cluster : 3 : 11 Cluster : 4 : 2 Cluster : 5 : 5 Cluster : 6 : 2 Cluster : 7 : 1 Cluster : 8 : 7 Cluster : 9 : 42 Cluster : 10 : 15 Cluster : 11 : 5 Cluster : 12 : 6 Cluster : 13 : 18 Cluster : 14 : 9 Cluster : 15 : 4 . label_map = {} for i in range(len(labels)): label_map[data_index[i]] = labels[i] . Let&#39;s take a look at some of the clusters. . Cluster 1 is the largest cluster with 98 incidents. For cluster 1 incidents we have rare types with intermittent peaks. . cluster_number = 1 display(Markdown(f&quot;### Codes for Cluster {cluster_number}&quot;)) for k,v in label_map.items(): if v == cluster_number: code = str(k).zfill(4) description = combined_typed[combined_typed[&quot;OFFENSE_CODE&quot;] == code][&quot;OFFENSE_DESCRIPTION&quot;].values[0] print(f&quot;{k} : {description}&quot;) . . Codes for Cluster 1 . 3300 : Migrated Report - Other Part III 3303 : NOISY PARTY/RADIO-ARREST 3302 : GATHERING CAUSING ANNOYANCE 3350 : Migrated Report - Injured/Medical/Sick Assist 3126 : WARRANT ARREST - OUTSIDE OF BOSTON WARRANT 3123 : EXPLOSIVES - TURNED IN OR FOUND 3100 : Migrated Report - Investigate Property 3203 : FIREARM/WEAPON - LOST 3200 : Migrated Report - Investigate Person 3000 : Migrated Report - Death Investigation 3016 : FIREARM/WEAPON - ACCIDENTAL INJURY / DEATH 3029 : PRISONER - SUICIDE / SUICIDE ATTEMPT 3403 : PROTECTIVE CUSTODY / SAFEKEEPING 1108 : FRAUD - WELFARE 1100 : Migrated Report - Fraud 1105 : FRAUDS - ALL OTHER 1831 : SICK ASSIST 1800 : Migrated Report - Drugs - Possession/Manufacturing/Distribute 1807 : DRUGS - CLASS D TRAFFICKING OVER 50 GRAMS 1864 : DRUGS - POSS CLASS D - INTENT MFR DIST DISP 1866 : DRUGS - POSS CLASS E INTENT TO MF DIST DISP 1863 : DRUGS - POSS CLASS D - MARIJUANA, ETC. 1400 : Migrated Report - Vandalism/Destruction of Property 1500 : Migrated Report - Weapons Violation 1502 : WEAPON - FIREARM - SALE / TRAFFICKING 1302 : PROPERTY - RECEIVING STOLEN 1000 : Migrated Report - Counterfeiting/Forgery 1002 : FORGERY OR UTTERING 1200 : Migrated Report - Embezzlement 1602 : PROSTITUTION - SOLICITING 1601 : PROSTITUTION 1603 : PROSTITUTION - ASSISTING OR PROMOTING 1605 : PROSTITUTION - COMMON NIGHTWALKER 1610 : HUMAN TRAFFICKING - COMMERCIAL SEX ACTS 1620 : HUMAN TRAFFICKING - INVOLUNTARY SERVITUDE 1901 : GAMBLING - BETTING / WAGERING 618 : LARCENY THEFT FROM COIN-OP MACHINE 641 : BREAKING AND ENTERING (B&amp;E) MOTOR VEHICLE 600 : Migrated Report - Other Larceny 650 : Migrated Report - Larceny From MV 637 : LARCENY IN A BUILDING UNDER $50 633 : LARCENY SHOPLIFTING UNDER $50 634 : LARCENY NON-ACCESSORY FROM VEH. UNDER $50 639 : LARCENY OTHER UNDER $50 627 : LARCENY IN A BUILDING $50 TO $199 629 : LARCENY OTHER $50 TO $199 624 : LARCENY NON-ACCESSORY FROM VEH. $50 TO $199 623 : LARCENY SHOPLIFTING $50 TO $199 736 : RECOVERED - MV RECOVERED IN BOSTON (STOLEN IN BOSTON) MUST BE SUPPLEMENTAL 700 : Migrated Report - Auto Theft 770 : AUTO THEFT - RECOVERED IN BY POLICE 714 : AUTO THEFT OTHER 300 : Migrated Report - Robbery 315 : ROBBERY - KNIFE - STREET 335 : ROBBERY - UNARMED - CHAIN STORE 339 : ROBBERY - UNARMED - STREET 338 : ROBBERY - UNARMED - RESIDENCE 334 : ROBBERY - UNARMED - BUSINESS 349 : ROBBERY ATTEMPT - KNIFE - BANK 800 : Migrated Report - Assault/Assault &amp; Battery 803 : A&amp;B ON POLICE OFFICER 531 : BREAKING AND ENTERING (B&amp;E) MOTOR VEHICLE (NO PROPERTY STOLEN) 530 : BREAKING AND ENTERING (B&amp;E) MOTOR VEHICLE 527 : B&amp;E RESIDENCE DAY - NO PROP TAKEN 547 : B&amp;E NON-RESIDENCE DAY - NO PROP TAKEN 500 : Migrated Report - Burglary/Breaking and Entering 511 : B&amp;E RESIDENCE NIGHT - ATTEMPT FORCE 400 : Migrated Report - Aggravated Assault/Aggravated Assault &amp; Battery 404 : A&amp;B HANDS, FEET, ETC. - MED. ATTENTION REQ. 403 : ASSAULT &amp; BATTERY D/W - OTHER 402 : ASSAULT &amp; BATTERY D/W - KNIFE 432 : ASSAULT D/W - KNIFE ON POLICE OFFICER 990 : Justifiable Homicide 112 : KILLING OF FELON BY POLICE 121 : MANSLAUGHTER - VEHICLE - NEGLIGENCE 122 : MANSLAUGHTER - NEGLIGENCE 123 : MANSLAUGHTER - NON-VEHICLE - NEGLIGENCE 100 : Migrated Report - Criminal Homicide 2671 : VIOLATION - HARASSMENT PREVENTION ORDER 2672 : BIOLOGICAL THREATS 2648 : BOMB THREAT 2617 : CONSPIRACY EXCEPT DRUG LAW 2618 : EXPLOSIVES - POSSESSION OR USE 2611 : ABDUCTION - INTICING 2664 : CUSTODIAL KIDNAPPING 2628 : OBSCENE PHONE CALLS 2657 : VIOLATION - CITY ORDINANCE 2608 : CHILD REQUIRING ASSISTANCE (FOMERLY CHINS) 2600 : Migrated Report - Other Part II 2605 : CONTRIBUTING TO DELINQUENCY OF MINOR 2606 : PRISONER ATTEMPT TO RESCUE 2609 : DRUGS - GLUE INHALATION 2631 : PROPERTY - CONCEALING LEASED 2636 : PRISONER ESCAPE / ESCAPE &amp; RECAPTURE 2910 : VAL - OPERATING AFTER REV/SUSP. 2950 : Migrated Report - Motor Vehicle Crash 2400 : Migrated Report - Affray/Disturbing the Peace/Disorderly Conduct 2500 : Migrated Report - Kidnapping . Cluster 8 has events that have dramatically increased since the end of 2019. Coinciding with the onset of the pandemic. . cluster_number = 8 display(Markdown(f&quot;### Codes for Cluster {cluster_number}&quot;)) for k,v in label_map.items(): if v == cluster_number: code = str(k).zfill(4) description = combined_typed[combined_typed[&quot;OFFENSE_CODE&quot;] == code][&quot;OFFENSE_DESCRIPTION&quot;].values[0] print(f&quot;{k} : {description}&quot;) . . Codes for Cluster 8 . 3005 : SICK ASSIST 1810 : DRUGS - POSSESSION/ SALE/ MANUFACTURING/ USE 1832 : SICK ASSIST - DRUG RELATED ILLNESS 801 : ASSAULT - SIMPLE 423 : ASSAULT - AGGRAVATED 2670 : HARASSMENT/ CRIMINAL HARASSMENT 2905 : VAL - VIOLATION OF AUTO LAW . Clusters 12,0,13 and 10 have events that have decreased since the end of 2019. Coinciding with the onset of the pandemic. . cluster_number = 0 display(Markdown(f&quot;### Codes for Cluster {cluster_number}&quot;)) for k,v in label_map.items(): if v == cluster_number: code = str(k).zfill(4) description = combined_typed[combined_typed[&quot;OFFENSE_CODE&quot;] == code][&quot;OFFENSE_DESCRIPTION&quot;].values[0] print(f&quot;{k} : {description}&quot;) . . 3301 : VERBAL DISPUTE 3006 : SICK/INJURED/MEDICAL - PERSON 802 : ASSAULT SIMPLE - BATTERY 413 : ASSAULT - AGGRAVATED - BATTERY 2629 : HARASSMENT . cluster_number = 12 display(Markdown(f&quot;### Codes for Cluster {cluster_number}&quot;)) for k,v in label_map.items(): if v == cluster_number: code = str(k).zfill(4) description = combined_typed[combined_typed[&quot;OFFENSE_CODE&quot;] == code][&quot;OFFENSE_DESCRIPTION&quot;].values[0] print(f&quot;{k} : {description}&quot;) . . Codes for Cluster 12 . 3208 : PROPERTY - MISSING 1843 : DRUGS - POSS CLASS B - INTENT TO MFR DIST DISP 1841 : DRUGS - POSS CLASS A - INTENT TO MFR DIST DISP 1848 : DRUGS - POSS CLASS D - INTENT TO MFR DIST DISP 2646 : LIQUOR/ALCOHOL - DRINKING IN PUBLIC 2619 : FUGITIVE FROM JUSTICE . cluster_number = 13 display(Markdown(f&quot;### Codes for Cluster {cluster_number}&quot;)) for k,v in label_map.items(): if v == cluster_number: code = str(k).zfill(4) description = combined_typed[combined_typed[&quot;OFFENSE_CODE&quot;] == code][&quot;OFFENSE_DESCRIPTION&quot;].values[0] print(f&quot;{k} : {description}&quot;) . . Codes for Cluster 13 . 3109 : SERVICE TO OTHER PD INSIDE OF MA. 3160 : FIRE REPORT - CAR, BRUSH, ETC. 3002 : ANIMAL CONTROL - DOG BITES - ETC. 1106 : FRAUD - CREDIT CARD / ATM FRAUD 1830 : DRUGS - SICK ASSIST - HEROIN 1844 : DRUGS - POSS CLASS C 1503 : WEAPON - OTHER - CARRYING / POSSESSING, ETC 1304 : PROPERTY - STOLEN THEN RECOVERED 1300 : STOLEN PROPERTY - BUYING / RECEIVING / POSSESSING 611 : LARCENY PICK-POCKET 522 : BURGLARY - RESIDENTIAL - NO FORCE 521 : BURGLARY - RESIDENTIAL - ATTEMPT 542 : BURGLARY - COMMERICAL - NO FORCE 2660 : OTHER OFFENSE 2632 : EVADING FARE 2906 : VAL - OPERATING UNREG/UNINS Â CAR 2914 : VAL - OPERATING W/O AUTHORIZATION LAWFUL 2405 : DISORDERLY CONDUCT . cluster_number = 10 display(Markdown(f&quot;### Codes for Cluster {cluster_number}&quot;)) for k,v in label_map.items(): if v == cluster_number: code = str(k).zfill(4) description = combined_typed[combined_typed[&quot;OFFENSE_CODE&quot;] == code][&quot;OFFENSE_DESCRIPTION&quot;].values[0] print(f&quot;{k} : {description}&quot;) . . Codes for Cluster 10 . 3304 : NOISY PARTY/RADIO-NO ARREST 3305 : DRUNKENNESS 3102 : INVESTIGATION FOR ANOTHER AGENCY 3170 : INTIMIDATING WITNESS 1845 : DRUGS - POSS CLASS D 1846 : DRUGS - POSS CLASS E 1874 : DRUGS - OTHER 1510 : WEAPON - FIREARM - OTHER VIOLATION 311 : ROBBERY - COMMERCIAL 560 : BURGLARY - OTHER - FORCE 2401 : AFFRAY 2204 : LIQUOR LAW VIOLATION 2007 : VIOLATION - RESTRAINING ORDER (NO ARREST) 2006 : VIOL. OF RESTRAINING ORDER W ARREST 2005 : CHILD ENDANGERMENT . Cluster 11 has events that have stayed relatively periodic since 2015. . cluster_number = 11 display(Markdown(f&quot;### Codes for Cluster {cluster_number}&quot;)) for k,v in label_map.items(): if v == cluster_number: code = str(k).zfill(4) description = combined_typed[combined_typed[&quot;OFFENSE_CODE&quot;] == code][&quot;OFFENSE_DESCRIPTION&quot;].values[0] print(f&quot;{k} : {description}&quot;) . . Codes for Cluster 11 . 3810 : M/V ACCIDENT - INVOLVING BICYCLE - INJURY 3811 : M/V ACCIDENT - INVOLVING BICYCLE - NO INJURY 3402 : ANIMAL INCIDENTS (DOG BITES, LOST DOG, ETC) 616 : LARCENY THEFT OF BICYCLE 706 : AUTO THEFT - MOTORCYCLE / SCOOTER . Not much has changed in Bicycle related incidents since 2015. We can also take a look at the plots of the incidents of a cluster separately to verify the clustering. . def plotCluster(data, labels, cluster_number): descriptions = {} for k,v in label_map.items(): if v == cluster_number: code = str(k).zfill(4) description = combined_typed[combined_typed[&quot;OFFENSE_CODE&quot;] == code][&quot;OFFENSE_DESCRIPTION&quot;].values[0] descriptions[k] = description cols = 3 rows = math.ceil(len(descriptions)/cols) fig, axs = plt.subplots(rows,cols,figsize=(20,8)) fig.suptitle(&#39;Cluster &#39; + str(cluster_number),fontsize=16) row_i=0 column_j=0 # For each label there is, # plots every series with that label for k,v in descriptions.items(): axs[row_i, column_j].plot(yx,data[data_index.index(k)],c=&quot;gray&quot;,alpha=0.4) axs[row_i, column_j].set_title(f&quot;{v}&quot;) locator = mdates.AutoDateLocator() formatter = mdates.ConciseDateFormatter(locator) axs[row_i, column_j].xaxis.set_major_locator(locator) axs[row_i, column_j].xaxis.set_major_formatter(formatter) column_j+=1 if column_j%cols == 0: row_i+=1 column_j=0 plt.show() . . plotCluster(data, labels, 11) . As a sanity check we can also view the raw data without normalization to check if the results are similar. . data_raw = np.array(list(count_series.values())) plotCluster(data_raw, labels, 11) . Thinking about the Data . I set out with the goal of using data visualization and analysis to convince a friend about th esafety of a certain area. However, to compare the data to the actual experience of living in an area is akin to spurious correlation. The data for example depends on a variety of factors. If staff was able to file the reports, funds allocated to different programs at the BPD, unemployment in an area, schools in an area. The data is influenced by a multitude of confounding factors. Property rates are often in a feedback loop with reports of crime. Where one influences the other. This curse of inferring causality from correlation is a common problem in data science. However we shall not be making that mistake here today. An exploration of the data is not enough to convince a friend but it is a stepping stone towards using data to ask questions of governance. For example, asking about the allocation of funds to different programs or asking the state to explain the reason for changes in the data. .",
            "url": "https://whoisankit.xyz/visualization/2022/07/17/rms-plot.html",
            "relUrl": "/visualization/2022/07/17/rms-plot.html",
            "date": " • Jul 17, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Solving "Infinite Loop" - Part 1",
            "content": "Nerd Sniped . Many years ago, I was the unsuspecting victim of a nerd sniping incident. I first came across Infinite Loop in 2015. An iOS and Android game Designed by Jonas Lekevicius. Jonas was inspired by the flash game &quot;Loops of Zen&quot; by Dr. Arend Hintze. In Jonas&#39; words, . Infinite Loop is a very simple, relaxing and never ending game for iPhone, iPad and Android. Your goal is to turn all forms on the grid so that there would be no open connections. Most levels are algorithmically generated and randomly colored, and after successfully completing each level you will be rewarded with a beautiful design of your own making. . Jonas is not wrong. The game is relaxing but also addictive. . . My girlfriend at the time was obsessed with the game. Before I knew it she was more than a thousand levels into the game. As a young undergraduate, I was competitive. To a fault. I had to find a way to play the game without playing the game. If I was going to hit level 1000, I needed to game the game. I spent about a month of actual work trying to figure out a backtracking algorithm to solve the board position and then figuring out how to connect the python script to my phone. Unfortunately an algorithm bug and life in general made me forget about the game and hitting level 1000. . It&#39;s 2022 now and I think it is high time I had a blog with an interesting post or two. This spring I took CS5800, Introduction to Algorithms by Dr. Abhi Shelat. Towards the end of the course we were introduced to the applications of Maximum Flow algorithms. I had an intense sense of deja vu when I read what was on Homework 10. Matching things adjacent to each other. Tiling dominoes on a grid. . . This Summer, I decided to give Solving Infinite Loop another shot. In Part 1 of this series, we will use basic image processing to get game tiles from a screenshot and explore algorithms to solve the game. In Part 2 , We will use some basic computer vision to make a more robust image to game tile converter. Let&#39;s begin! . Introduction . An arbitray level of Infinite Loop is composed of a rectangular grid (Rows and Columns, 2D Array/List) of tiles. We have 5 types of tiles with different number of outgoing edges (the degree of a tile). For tiles with degree 2, there are 2 types of tiles possible. Each tile can of course be rotated along its center. Different tiles have different number of possible rotations. . . Let us try to define a couple of helper functions to make our lives easier. We will first define a function that will list out all the possible rotations of a tile. We represent each tile with a 4 bit binary number going clockwise. A tile of degree 1 with its edge pointing up is represented as 1000. The different rotations of this tile are obtained by left or right shifting the number till we get the original number. For 1000, we get 1000, 0001, 0010 and 0100. . def cycleOrientations(orientation): &quot;&quot;&quot; Takes a list as input and returns a list of all possible rotations of the input list. &quot;[a,b,c]&quot; -&gt; &quot;[a,b,c],[b,c,a],[c,a,b]&quot; &quot;&quot;&quot; out = [] out.append(orientation) # let t be the first orientation t = orientation.copy() # add the first element of the list to the end t.append(t.pop(0)) # while t is not the same as the first orientation # repeat the above operation and add it to the # list of possible orientations while t != orientation: out.append(t.copy()) t.append(t.pop(0)) return out print(&quot; &quot;1010 &quot; : &quot;, cycleOrientations(list(&quot;1010&quot;))) print(&quot; &quot;0001 &quot; : &quot;, cycleOrientations(list(&quot;0001&quot;))) print(&quot; &quot;1111 &quot; : &quot;, cycleOrientations(list(&quot;1111&quot;))) . &#34;1010&#34; : [[&#39;1&#39;, &#39;0&#39;, &#39;1&#39;, &#39;0&#39;], [&#39;0&#39;, &#39;1&#39;, &#39;0&#39;, &#39;1&#39;]] &#34;0001&#34; : [[&#39;0&#39;, &#39;0&#39;, &#39;0&#39;, &#39;1&#39;], [&#39;0&#39;, &#39;0&#39;, &#39;1&#39;, &#39;0&#39;], [&#39;0&#39;, &#39;1&#39;, &#39;0&#39;, &#39;0&#39;], [&#39;1&#39;, &#39;0&#39;, &#39;0&#39;, &#39;0&#39;]] &#34;1111&#34; : [[&#39;1&#39;, &#39;1&#39;, &#39;1&#39;, &#39;1&#39;]] . Getting Tiles from an Image . Onto extracting tiles from an image. For an image of a game level, we want a 2D array of tiles. Let us look at a screenshot. . . This level is a 11x6 grid. Each tile is square of the same side length. There are three colors in the image. A background color, a tile color and a faint line color used for the UI icon. We can say for cetrain that the most frequent color in the image is the backround color and most non trivial game levels have more pixels of the tile color than the UI icon color. We will define a function that takes as input an image path and returns a black and white image with only the tile color colored white and the background color black. . # for basic image processing from PIL import Image,ImageDraw # for fast math import numpy as np import math # to display images and graphs import matplotlib.pyplot as plt # for nodes and edges import networkx as nx # for io stuff import io print(&#39;Importing PIL, numpy, matplotlib, networkx&#39;) im_path = &quot;./imgs/1/20220604-003503.png&quot; . . Importing PIL, numpy, matplotlib, networkx . def screenshotToBw(path): &quot;&quot;&quot; Take an input image and return a black and white image &quot;&quot;&quot; # open the image im = Image.open(path) # convert it to grayscale im = im.convert(&#39;L&#39;) # count the unique values unique, counts = np.unique( im.__array__(), return_counts=True ) # sort values and counts Z = [x for _,x in sorted(zip(counts,unique))] # select the 2 most common values b,w = Z[-2:] # set the first value # as the thresholding tile color thresh = b # for each pixel if the pixel within # 2 values of the threshold set it to # white else set it to black im = im.point( lambda x : 255 if abs(x - b) &lt; 2 else 0, mode=&#39;1&#39; ) return im screenshotToBw(im_path).resize((288,512),Image.NEAREST) . . Thanks to the black background we can find the bounding box of the game. PIL has a handy function getbbox that returns the bounding box of a black and white image. This bounding box is not exactly the size of the grid. However, if we assume that a tile is of a constant side length and the game is always aligned to the center of the screenshot, we can infer the number of columns and rows of the grid. If the tile side length is assumed to be C, then the width of the grid should be columns x C and the height rows x C. Thus inversely, the number of columns is ceil(width of the bbox / C) and the number of rows is ceil(height of the bbox / C). Armed with this insight we can define a function that takes as input a black and white image and returns the a 2D array of bounding boxes for each tile. . def bwToBboxes(im,C=134): &quot;&quot;&quot; Converts a black and white image and a size length to a list of bounding boxes Caveat : Here we assume that the image has a game board aligned with the midpoint of the image and has a known tile side-length. &quot;&quot;&quot; # get image bounding box bbox = im.getbbox() # get the image width and height w, h = im.size # get the bbox width and height W = bbox[2] - bbox[0] H = bbox[3] - bbox[1] # get the number of rows and columns Col,Row = math.ceil(W/C),math.ceil(H/C) # calculate the center of the image mid_x = w/2 mid_y = h/2 # create an empty list of bounding boxes boxes = [] for row in range(Row): t = [] for col in range(Col): # calculate the top left coordinates for each bounding box # If the coordinates of the center are (mid_x,mid_y) # and the game is center aligned the top left corner of the game # has to be 1/2 the width and height of the # game board from the center of the image p_x = (mid_x - (Col * C / 2)) + col * C p_y = (mid_y - (Row * C / 2)) + row * C t.append([int(p_x),int(p_y),int(p_x+C),int(p_y+C)]) boxes.append(t) return boxes . . Let&#39;s take a look at the boxes returned by the function. . def drawBoxes(im,boxes): &quot;&quot;&quot; Draws bounding boxes on an image &quot;&quot;&quot; # create a new image im_out = Image.new(im.mode,im.size) # create a draw object draw = ImageDraw.Draw(im_out) # copy the image im_out.paste(im) # draw the bounding boxes for row in boxes: for box in row: draw.rectangle(box,outline=&#39;red&#39;,width=2) return im_out im = screenshotToBw(im_path) boxes = bwToBboxes(im) drawBoxes(im,boxes).resize((576,1024),Image.NEAREST) . . Now that we have individual bounding boxes we can take a look at the individual tiles for the pixels on the midpoints of the north, east, south and west edges. . A cropped image of a tile at position 1,1 looks like this. . im_cropped = im.crop(boxes[1][0]) display(im_cropped) print(&quot;Size of the tile : &quot; , im_cropped.size) print(&quot;Pixel at North corner : &quot; , im_cropped.getpixel((im_cropped.width/2,0))) print(&quot;Pixel at East corner : &quot; , im_cropped.getpixel((im_cropped.width-1,im_cropped.height/2))) print(&quot;Pixel at South corner : &quot; , im_cropped.getpixel((im_cropped.width/2,im_cropped.height-1))) print(&quot;Pixel at West corner : &quot; , im_cropped.getpixel((0,im_cropped.height/2))) . . Size of the tile : (134, 134) Pixel at North corner : 0 Pixel at East corner : 255 Pixel at South corner : 0 Pixel at West corner : 0 . Based on the values at each pixel position we can begin to construct binary representations of the tiles. . def getTiles(im,C=134): &quot;&quot;&quot; Takes an image and a tile size and returns a 2D list of tiles &quot;&quot;&quot; tiles = [] # use the bwToBboxes function to get the bounding boxes bboxes = bwToBboxes(im,C) for i,row in enumerate(bboxes): # create a temporary row v = [] for j,box in enumerate(row): # for each bounding box crop the image # and get the NESW pixel values t = [] cropjorts = im.crop(box) pxN = cropjorts.getpixel((int(C/2),0)) pxS = cropjorts.getpixel((int(C/2),C-1)) pxE = cropjorts.getpixel((C-1,int(C/2))) pxW = cropjorts.getpixel((0,int(C/2))) # append 1 if the pixel is white else 0 t.append(1 if pxN == 255 else 0) t.append(1 if pxE == 255 else 0) t.append(1 if pxS == 255 else 0) t.append(1 if pxW == 255 else 0) v.append(t) tiles.append(v) return tiles . . Thus for the game image we have the following tiles: . smol_image = im.crop(im.getbbox()) w,h = smol_image.size r = w/h t_H = 500 t_W = int(t_H * r) display(smol_image.resize((t_W,t_H),Image.NEAREST)) tiles = getTiles(im) for row in tiles: for tile in row: print(&quot;&quot;.join(map(str,tile)),end=&#39; &#39;) print() . . 0000 0010 1001 0000 0011 0011 0100 0101 0110 0100 1100 1010 1000 0000 0100 0011 1101 1001 1010 0011 1011 0111 1101 0001 0101 1010 0010 0001 1100 1011 0111 1110 0101 1110 0110 0100 1010 0100 0101 0110 0110 1001 1100 1011 0101 1010 0011 0010 0000 1010 0000 0000 1100 0001 0100 1101 0101 1000 0110 0010 0000 0000 0100 0010 1100 0100 . Developing an Algorithm . Now that we have game tiles from the screenshot, we can begin developing an algorithm. An approach is to select a possible tile orientation and check if it is part of a valid solution and incrementally add tiles to the solution. If we find an invalid tile we try another option, if there are no available options we backtrack to the previously added tile and try another option from the list of possible rotations and move on to the next tile. . I know that sounds complicated but what we are trying to do can be imagined as a large tape of tapes with pointers. Each time we try an option we move the pointer forward and if we would like to undo we move the pointer back. We can also set the pointer back to the beginning of the tape. This sounds a lot like a data structure begging to be implemented. So let&#39;s get to implementing it. . The BigTape class . The BigTape class implementation can be described as follows. . Properties data , a list of items of any data type | current , an integer that points to the current position of the pointer | length , the length of the data list | . | Methods add(item) , add an item to the tape | addItems(items) , add a list of items to the tape | select() , return the item at the current position of the pointer and increment the pointer | undo() , move the pointer back one position | top() , return the item at the current position of the pointer without incrementing the pointer | reset() , set the pointer to the beginning of the tape | . | class BigTape: # create a tape object with an empty list # and a tape length and current position of 0 def __init__(self): self.data = [] self.current = 0 self.length = 0 # add a value to the list # and increment the tape length def add(self,item): self.data.append(item) self.length += 1 # add lots of values to the list def addItems(self,items): for i in items: self.add(i) # get the value at the current position # and increment the current position # if the current position is greater than the length # of the tape return None def select(self): if self.length &gt; 0 and self.current &lt; self.length: out = self.data[self.current] self.current += 1 return out else : return None # get the value at the current position # if the current position is greater than the length # of the tape return None def top(self): if self.length &gt; 0 and self.current &lt; self.length: out = self.data[self.current] return out else : return None # decrement the current position def undo(self): if self.length &gt; 0 : self.current -= 1 # set the current position to 0 def reset(self): self.current = 0 . . Pseudocode . Let&#39;s now write the algorithm in pseudocode. . tileTape, a BigTape object that holds the different valid tile orientations as BigTape objects | While we still have tiles available to select in the tileTape Select a tile | If the tile has available orientations Select an orientation | Check if the orientation is valid | If it is valid Add the tile orientation to the solution | | If it is not valid Undo the tileTape | | | If there are no more orientations available for the tile Reset the orientation tape of the selected tile | Undo the tileTape | Undo the tileTape | Remove the last tile from the solution | | | Valid Orientations . Valid orientations are orientations that can be placed on the board. For example, a tile on the west edge of the game board cannot face west because there is no tile to connect to in the west. . We can visualize the entire set of tiles as a graph with nodes and edges. Let us begin with a grid graph with the same shape as the tiles array. . rows, cols = len(tiles),len(tiles[0]) G = nx.grid_2d_graph(rows,cols) pos = {(x,y):(y,-x) for x,y in G.nodes()} plt.figure(3,figsize=(len(tiles[0]),len(tiles))) nx.draw(G, pos, with_labels=True, node_size=1300,node_color=&#39;0.7&#39;,edge_color=&#39;black&#39;) . . Let us now remove nodes with empty tiles. An empty tile has the bit pattern 0000. . for i,row in enumerate(tiles): for j,tile in enumerate(row): if sum(tile) == 0: G.remove_node((i,j)) pos = {(x,y):(y,-x) for x,y in G.nodes()} plt.figure(3,figsize=(len(tiles[0]),len(tiles))) nx.draw(G, pos, with_labels=True, node_size=1300,node_color=&#39;0.7&#39;,edge_color=&#39;black&#39;) . . For tile (3,0), the tile is a 1000 tile and from the graph it is obvious that the only valid sets of orientations are 1000 , 0100 and 0010. We can represent these as edge lists. In this case we have two edge lists for the two valid orientations. The first edge list is (3,0) to (2,0). The second edge list is (3,0) to (3,1). Finally we have (3,0) to (4,0). Let&#39;s write a couple of functions to list out the valid neighbour lists for each tile. . def getNeighbours(r,c): &quot;&quot;&quot; Returns a list of neighbours of a tile (1,0) -&gt; (0,1),(1,1),(2,0),(1,-1) &quot;&quot;&quot; return [(r-1,c),(r,c+1),(r+1,c),(r,c-1)] def selectNeighbors(G,V,orientation): &quot;&quot;&quot; Select the neighbours of a tile in a given orientation if the orientation is not valid return an empty list V is a tuple (row,col) G is a grid graph orientation is a string &#39;N&#39;,&#39;E&#39;,&#39;S&#39;,&#39;W&#39; (1,0) , &quot;0001&quot; -&gt; [] (1,0) , &quot;0010&quot; -&gt; [((1,0)(2,0))] &quot;&quot;&quot; r,c = V neighbours = getNeighbours(r,c) out = [] for i,bit in enumerate(orientation): if bit: if neighbours[i] in G.adj[V]: out.append((V,neighbours[i])) else: return [] return out def getOptions(G,tiles,V): &quot;&quot;&quot; Get the edge list options for each tile V is a tuple (row,col) G is a grid graph tiles is a 2D list of tiles &quot;&quot;&quot; r,c = V # get all possible orientations of a tile orientations = cycleOrientations(tiles[r][c]) out = [] # for each orientation get the neighbours in that orientation # and add them to the list of options for that tile for o in orientations: n = selectNeighbors(G,V,o) if n: out.append(n) return out . . Let&#39;s populate some dictionaries to keep track of the degree of each tile and the valid neighbour lists for each tile. . degreeDict = {} for v in G.nodes(): r,c = v s = sum(tiles[r][c]) degreeDict[v] = s optionDict = {} for v in G.nodes(): optionDict[v] = getOptions(G,tiles,v) . The valid options we have for tile 2,0 are, . print(&quot;Edge options : &quot;, optionDict[(2,0)]) print(&quot;Number of valid options : &quot;, len(optionDict[(2,0)])) print(&quot;Degree : &quot;, degreeDict[(2,0)]) . Edge options : [[((2, 0), (1, 0))], [((2, 0), (3, 0))]] Number of valid options : 2 Degree : 1 . Let&#39;s now write a function to check if a set of options is valid or not. . def check(opList, opT, G, degreeDict): &quot;&quot;&quot; Check if adding opT to opList results in a valid game configuration opT is an edge list opList is a list of edge lists &quot;&quot;&quot; tempG = nx.Graph() tempG.add_nodes_from(G.nodes()) # add the edges from opList to a temp graph for op in opList: for edge in op: tempG.add_edge(edge[0],edge[1]) # add the edges from opT to the temp graph for edge in opT: tempG.add_edge(edge[0],edge[1]) flag = True # check if the degree limits on each node are within bounds for v in tempG.nodes(): if degreeDict[v] &lt; tempG.degree[v]: flag = False return flag break return flag . Running the Algorithm . We can now put it all together and write the final algorithm in code and see if it works. . tileTape = BigTape() # add our options to the tile tape for i in optionDict: ops = optionDict[i] tile = BigTape() for j in ops: tile.add(j) tileTape.add(tile) iton = list(G.nodes()) ntoi = {v:i for i,v in enumerate(iton)} solution = [] sol_l = [] while tileTape.top(): n = tileTape.current t = tileTape.select() if t.top(): op = t.select() f = check(solution,op,G,degreeDict=degreeDict) sol_l.append({ &quot;op&quot;:op.copy(), &quot;valid&quot;:f, &quot;sol&quot;:solution.copy(), &quot;n&quot; : n }) if f: solution.append(op) continue else: tileTape.undo() else: t.reset() tileTape.undo() tileTape.undo() solution.pop() tempG = nx.Graph() tempG.add_nodes_from(G.nodes()) for op in solution: for edge in op: tempG.add_edge(edge[0],edge[1]) pos = {(x,y):(y,-x) for x,y in tempG.nodes()} fig = plt.figure(3,figsize=(len(tiles[0]),len(tiles))) nx.draw(tempG, pos, with_labels=True, node_size=1300, node_color=&#39;0.7&#39;, edge_color=&#39;blue&#39;, width=2) . Et Voila! It does. We have a backtracking algorithm that can solve arbitrary game levels. I know I blitzed through a lot of code so a nice little animation showing the algorithm as it works would be super refreshing :green_apple: . So here&#39;s an animation of the algorithm running on the level above. The number on the bottom left corner of the animation is the number of the option being tried. The numbers near the vertices are the required out degree of the vertex. Edges are colored green if they are a valid option and red if they are not. If green the edges are added to the partial solution. The red circle around the vertex is the current tile being tried. . def drawSol(sol,oP,f,i,iton,tiles,G,ax=None): tempG = nx.Graph() tempG.add_nodes_from(G.nodes()) for op in sol: for edge in op: tempG.add_edge(edge[0],edge[1]) pos = {(x,y):(y,-x) for x,y in tempG.nodes()} fig, ax = plt.subplots(figsize=(len(tiles[0]),len(tiles))) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;bottom&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) nx.draw_networkx_edges(G,pos,ax=ax,edge_color=&#39;white&#39;,width=2) nx.draw_networkx_edges(tempG,pos,ax=ax,edge_color=&#39;blue&#39;,width=2) # draw edges in op as red col = &#39;green&#39; if f else &#39;red&#39; for edge in oP: ax.plot([pos[edge[0]][0],pos[edge[1]][0]],[pos[edge[0]][1],pos[edge[1]][1]],color=col) currentNode = iton[i] # draw red transparent circle around current node ax.plot(pos[currentNode][0],pos[currentNode][1],color=&#39;red&#39;,marker=&#39;o&#39;,markersize=10,alpha=0.5) nx.draw_networkx_nodes(tempG,pos=pos,node_size=50,node_color=&#39;0.7&#39;,ax=ax) # write degree dict values for each node for v in tempG.nodes(): ax.text(pos[v][0] + 0.15 ,pos[v][1] + 0.15 ,str(degreeDict[v]) ,color=&#39;black&#39;,fontsize=10) buf = io.BytesIO() fig.savefig(buf) buf.seek(0) plt.close(fig) return Image.open(buf) imgs = [] for n,i in enumerate(sol_l): i = drawSol(i[&quot;sol&quot;],i[&quot;op&quot;],i[&quot;valid&quot;],i[&quot;n&quot;],iton,tiles,G) # add frame number in bottom left draw = ImageDraw.Draw(i) draw.text((0,i.size[1]-10),str(n),fill=(0,0,0)) imgs.append(i) imgs[0].save(&quot;./imgs/1/out.gif&quot;, save_all=True, append_images=imgs[1:],duration=1000, loop=0) . . Graph to Tiles . Now that we have the final graph configuration for the game level, we can use the graph to generate the final tile bit patterns. Using the bit patterns we can find out how many rotations of the tile are needed to get to the solution. . def graphToTiles(G,r,c): # for each node in the graph check its 2d neighbours and add them to the tile out = {} for v in G.nodes(): x,y = v neigh = getNeighbours(x,y) b = [] for n in neigh: if n in G.adj[v]: b.append(1) else: b.append(0) out[v] = b return out # calculate right rotate ditance of two binary strings def compare(fromList,toList): i = 0 fl = fromList.copy() tl = toList.copy() while fl != tl: fl.append(fl.pop(0)) i += 1 if i&gt;4: return 0 break return i def shiftsFromTiles(fromTiles,toTiles): r = len(fromTiles) c = len(fromTiles[0]) out = [] for i in range(r): t = [] for j in range(c): value = compare(fromTiles[i][j],toTiles[i][j]) t.append(value) out.append(t) return out def mergeTiles(tileImages,tiles): r,c = len(tiles),len(tiles[0]) t_w, t_h = tileImages[0][0].size out = Image.new(&#39;RGB&#39;, (t_w*c, t_h*r)) for i,row in enumerate(tileImages): for j,tile in enumerate(row): out.paste(tile, (j*t_w, i*t_h)) return out . . tiles = getTiles(im) solutionTiles = graphToTiles(tempG,r,c) toTiles = [] for r,row in enumerate(tiles): row_ = [] for c,tile in enumerate(row): if (r,c) in solutionTiles: row_.append(solutionTiles[(r,c)]) else: row_.append([0]*len(tile)) toTiles.append(row_) shifts = shiftsFromTiles(tiles,toTiles) . . tileImages = [[im.crop(box) for box in row] for row in boxes] solutionImages = [[im.crop(box) for box in row] for row in boxes] for i,row in enumerate(solutionImages): for j,tile in enumerate(row): if (i,j) in solutionTiles: solutionImages[i][j] = tile.rotate(shifts[i][j]*90) . problem = mergeTiles(tileImages,tiles) solved = mergeTiles(solutionImages,tiles) plt.figure(figsize=(10,10)) plt.subplot(1,2,1) plt.imshow(problem) plt.axis(&#39;off&#39;) plt.subplot(1,2,2) plt.imshow(solved) plt.axis(&#39;off&#39;) plt.show() . . The python library that connects with the game is called AndroidViewClient which connects to a running adb server and is able to simulate events such as taking a screenshot, touching the screen, etc. All the things we need to interact with the game. . Here&#39;s a smol timelapse of the game being played by the bot. Look Ma No Hands! . Can we use Maximum Network Flow? . Back to CS5800. The first problem on Homework 10 - &quot;Given a grid of square tiles where some are colored black and some are blank, find if it is possible to cover the tiles with a set of domino tiles, where each domino tile is made of two adjacent square tiles i.e. is a rectangle&quot;. Prof. Shelat is usually kind enough to include hints on how to solve a problem. For this problem he advised us to look at it as a maximum bipartite matching problem. A problem that can be transformed into an instance of the max network flow problem. . . What is a Maximum Bipartite Matching? . A Bipartite matching is a set of edges from one source set of vertices to another target set of vertices such that no two edges share the same source vertex and no two edges share the same target vertex. This means each item in the source set is matched to exactly one item in the target set. A maximum bipartite matching is a bipartite matching with the maximum number of edges. Meaning if we add a new edge to the matching it will cease to be a bipartite matching. . . source . Let us now try to convert a game level into a bipartite matching problem. We color vertices in a checkerboard pattern. Tiles with odd sums are colored magenta and the rest are cyan. Our tile graph looks like this, . tG = nx.Graph() tG.add_nodes_from(G.nodes()) pos = {(x,y):(y,-x) for x,y in tempG.nodes()} fig, ax = plt.subplots(figsize=(len(tiles[0]),len(tiles))) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;bottom&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) nx.draw_networkx_edges(G,pos,ax=ax,edge_color=&#39;0.7&#39;,width=2) # color vertex cyan if sum of vertex coordinates is even colors = [&#39;c&#39; if sum(x)%2 == 0 else &#39;m&#39; for x in G.nodes()] nx.draw_networkx_nodes(tG,pos=pos,node_size=50,node_color=colors,ax=ax) buf = io.BytesIO() fig.savefig(buf) buf.seek(0) plt.close(fig) Image.open(buf) . . We can move the sets over to the left and right to see them in the usual bipartite matching format. . setC = [ n for n in G.nodes() if sum(n)%2 == 0 ] setM = [ n for n in G.nodes() if sum(n)%2 == 1 ] colors = {n: &#39;c&#39; if n in setC else &#39;m&#39; for n in G.nodes()} tDG = nx.DiGraph() tDG.add_nodes_from(tG.nodes()) # draw edges from setC to setM for n in setC: for m in setM: if m in G.adj[n]: tDG.add_edge(n,m) #position of setC nodes on the left of screen and setM nodes on the right for i,n in enumerate(setC): pos[n] = (0,i) for i,n in enumerate(setM): pos[n] = (1,i) #pos = {(x,y):(y,-x) for x,y in tDG.nodes()} fig, ax = plt.subplots(figsize=(len(tiles[0]),len(tiles))) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;bottom&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) nx.draw_networkx_edges(tDG,pos,ax=ax,edge_color=&#39;0.7&#39;,width=2) # color vertex cyan if sum of vertex coordinates is even colors = [&#39;c&#39; if sum(x)%2 == 0 else &#39;m&#39; for x in G.nodes()] nx.draw_networkx_nodes(tDG,pos=pos,node_size=50,node_color=colors,ax=ax) buf = io.BytesIO() fig.savefig(buf) buf.seek(0) plt.close(fig) Image.open(buf) . . Wow what a mess, the grid layout with directed edges should be much easier to visualize. . setC = [ n for n in G.nodes() if sum(n)%2 == 0 ] setM = [ n for n in G.nodes() if sum(n)%2 == 1 ] colors = {n: &#39;c&#39; if n in setC else &#39;m&#39; for n in G.nodes()} tDG = nx.DiGraph() tDG.add_nodes_from(tG.nodes()) # draw edges from setC to setM for n in setC: for m in setM: if m in G.adj[n]: tDG.add_edge(n,m) pos = {(x,y):(y,-x) for x,y in tDG.nodes()} fig, ax = plt.subplots(figsize=(len(tiles[0]),len(tiles))) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;bottom&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) nx.draw_networkx_edges(tDG,pos,ax=ax,edge_color=&#39;0.7&#39;,width=2) # color vertex cyan if sum of vertex coordinates is even colors = [&#39;c&#39; if sum(x)%2 == 0 else &#39;m&#39; for x in G.nodes()] nx.draw_networkx_nodes(tDG,pos=pos,node_size=50,node_color=colors,ax=ax) buf = io.BytesIO() fig.savefig(buf) buf.seek(0) plt.close(fig) Image.open(buf) . . What is Maximum Network Flow? . For any graph G the max network flow is the maximum amount of flow that can be sent from one vertex to another on the graph. Usually a flow network with source s and target t follows the following rules: . The sum of flows going into a vertex must equal the sum of flows going out of that vertex for all vertices except s and t. | The sum of flows going out of s must equal the sum of flows going into t. | The flow going through an edge must be less than or equal to the capacity of that edge. | . source . Bipartite Matching to max Network Flow . To convert a bipartite matching problem to a max network flow problem we add a new vertex s and a new vertex t to the graph. We then add an edge from s to each of the vertices in the source set and an edge from each of the vertices in the target set to t. We then add edges between vertices in the source set and adjacent vertices in the target set. Once we solve for the max network flow, we can recover the bipartite matching that results in the max network flow. . . So all we need to do is add a source and sink to our directed graph. . setC = [ n for n in G.nodes() if sum(n)%2 == 0 ] setM = [ n for n in G.nodes() if sum(n)%2 == 1 ] tDG = nx.DiGraph() tDG.add_nodes_from(tG.nodes()) # draw edges from setC to setM for n in setC: for m in setM: if m in G.adj[n]: tDG.add_edge(n,m) pos = {(x,y):(y,-x) for x,y in tDG.nodes()} tDG.add_node(&#39;s&#39;) tDG.add_node(&#39;t&#39;) pos[&#39;s&#39;] = (-2,-len(tiles)//2+1) pos[&#39;t&#39;] = (len(tiles[0])+1,-len(tiles)//2+1) for n in setC: tDG.add_edge(&#39;s&#39;,n) for n in setM: tDG.add_edge(n,&#39;t&#39;) fig, ax = plt.subplots(figsize=(len(tiles[0])+4,len(tiles))) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;bottom&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) colors = [] for n in tDG.nodes(): if n == &#39;s&#39;: colors.append(&#39;g&#39;) elif n == &#39;t&#39;: colors.append(&#39;b&#39;) elif sum(n)%2 == 0: colors.append(&#39;c&#39;) else: colors.append(&#39;m&#39;) nx.draw_networkx_nodes(tDG,pos=pos,node_size=50,node_color=colors,ax=ax) edge_colors = [] edge_weights = [] edge_styles = [] for e in tDG.edges(): if e[0] == &#39;s&#39;: edge_colors.append(&#39;c&#39;) edge_weights.append(0.5) edge_styles.append(&#39;dashed&#39;) elif e[1] == &#39;t&#39;: edge_colors.append(&#39;m&#39;) edge_weights.append(0.5) edge_styles.append(&#39;dashed&#39;) else: edge_colors.append(&#39;gray&#39;) edge_weights.append(1) edge_styles.append(&#39;solid&#39;) nx.draw_networkx_edges(tDG,pos=pos,ax=ax,edge_color=edge_colors,width=edge_weights,style=edge_styles) buf = io.BytesIO() fig.savefig(buf) buf.seek(0) plt.close(fig) Image.open(buf) . . Does it work? aka Why does it not work? . Now we take the edges from the source to the source set and set their capacity equal to their out degree as opposed to unit capacity in the previous domino matching problem. . Once we perform the flow transformation and recover the matching, The Max Flow solution looks like this, . attrs = {} for e in tDG.edges(): if e[0] == &#39;s&#39;: r,c = e[1] attrs[e] = {&#39;capacity&#39;:sum(tiles[r][c])} elif e[1] == &#39;t&#39;: r,c = e[0] attrs[e] = {&#39;capacity&#39;:sum(tiles[r][c])} else: attrs[e] = {&#39;capacity&#39;:1} nx.set_edge_attributes(tDG,attrs) flow_out = nx.maximum_flow(tDG,&#39;s&#39;,&#39;t&#39;) out_graph = nx.Graph() out_graph.add_nodes_from(G.nodes()) for e in flow_out[1]: if e != &#39;s&#39; and e != &#39;t&#39;: for e2 in flow_out[1][e]: if e2 != &#39;s&#39; and e2 != &#39;t&#39; and flow_out[1][e][e2] &gt; 0: out_graph.add_edge(e,e2) pos = {(x,y):(y,-x) for x,y in out_graph.nodes()} fig, ax = plt.subplots(figsize=(len(tiles[0]),len(tiles))) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;bottom&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) nx.draw_networkx_edges(out_graph,pos,ax=ax,edge_color=&#39;0.7&#39;,width=2) # color vertex cyan if sum of vertex coordinates is even colors = [&#39;c&#39; if sum(x)%2 == 0 else &#39;m&#39; for x in G.nodes()] nx.draw_networkx_nodes(out_graph,pos=pos,node_size=50,node_color=colors,ax=ax) buf = io.BytesIO() fig.savefig(buf) buf.seek(0) plt.close(fig) maxFlowIm = Image.open(buf) pos = {(x,y):(y,-x) for x,y in tempG.nodes()} fig, ax = plt.subplots(figsize=(len(tiles[0]),len(tiles))) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;bottom&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) nx.draw_networkx_edges(tempG,pos,ax=ax,edge_color=&#39;0.7&#39;,width=2) # color vertex cyan if sum of vertex coordinates is even colors = [&#39;c&#39; if sum(x)%2 == 0 else &#39;m&#39; for x in G.nodes()] nx.draw_networkx_nodes(tempG,pos=pos,node_size=50,node_color=colors,ax=ax) buf = io.BytesIO() fig.savefig(buf) buf.seek(0) plt.close(fig) backTrackIm = Image.open(buf) fig = plt.figure(figsize=(10,10)) ax1 = plt.subplot(1,2,1) plt.imshow(backTrackIm) ax1.set_title(&#39;Backtrack&#39;) plt.axis(&#39;off&#39;) ax2 = plt.subplot(1,2,2) plt.imshow(maxFlowIm) ax2.set_title(&#39;Max Flow&#39;) plt.axis(&#39;off&#39;) plt.show() . . Converting to images and comparing the backtracking and max flow solutions, we see that the backtracking solution is obviously a valid solution and unfortunately the max flow solution is not. . solMaxFlowTiles = graphToTiles(out_graph,r=len(tiles),c=len(tiles[0])) toMaxFlowTiles = [] for r,row in enumerate(tiles): row_ = [] for c,tile in enumerate(row): if (r,c) in solMaxFlowTiles: row_.append(solMaxFlowTiles[(r,c)]) else: row_.append([0]*len(tile)) toMaxFlowTiles.append(row_) shiftsMaxFlow = shiftsFromTiles(tiles,toMaxFlowTiles) solutionMaxFlowImages = [[im.crop(box) for box in row] for row in boxes] for i,row in enumerate(solutionMaxFlowImages): for j,tile in enumerate(row): if (i,j) in solutionTiles: solutionMaxFlowImages[i][j] = tile.rotate((shiftsMaxFlow[i][j])*90) solved = mergeTiles(solutionImages,tiles) solvedMaxFlow = mergeTiles(solutionMaxFlowImages,tiles) plt.figure(figsize=(10,10)) ax1 = plt.subplot(1,2,1) plt.imshow(solved) ax1.set_title(&#39;Backtrack&#39;) plt.axis(&#39;off&#39;) ax2 = plt.subplot(1,2,2) plt.imshow(solvedMaxFlow) ax2.set_title(&#39;Max Flow&#39;) plt.axis(&#39;off&#39;) plt.show() . . Why is this happening? Parts of the solution look alright but the rest seems like a garbled mess. . Why? The maximum flow algorithm is a greedy algorithm and it selects paths on the graph that are the best at any one iteration of the algorithm. What we need is a smarter path finding algorithm which takes into account the orientation of the tiles in addition to the capacity of the edges. . I will be contacting Prof. Shelat to see if he has any suggestions for a better path finding algorithm for this problem. . What else can we use? . Due to the nature of the Puzzle it is similar to Sudoku and the 8-Queens puzzle. We can use the same techniques to solve these puzzles. Sudoku and 8-Queens are well known examples of the exact cover problem. So let&#39;s us try transforming Infinite Loop into an exact cover problem. . Exact Cover . The Exact Cover Problem can be stated simply as follows: . &quot;Given a set of subsets of a set of elements X, find a subset of the set such that each element in X is in exactly one subset selected.&quot; . For example1, Consider a set X = {A,B,C,D} and a set of subsets S = [{A,C}, {B,C}, {B,D}] then [{A,C} , {B,D}] is an exact cover of X. We can also visualize this as a bit matrix where each row represents a subset and each column represents an element in X. Finding an exact cover would be equivalent to finding a set of rows such that each column has only one bit. . . . https://en.wikipedia.org/wiki/Exact_cover&#8617; . | Transformation to Exact Cover . For Infinite Loop, I was conveniently able to find this YouTube video from a small time creator. In which they describe the precise transformation to convert Infinite Loop to an Exact Cover problem. . AlgorithmX . Algorithm X is an algorithm for solving the exact cover problem. It is a straightforward recursive, nondeterministic, depth-first, backtracking algorithm used by Donald Knuth to demonstrate an efficient implementation called DLX, which uses the dancing links technique. 1 . Here&#39;s a really nice lecture by Donald Knuth about Dancing Links. . . https://en.wikipedia.org/wiki/Knuth%27s_Algorithm_X&#8617; . | A nice write up by Ali Assaf from Ecole Polytechnique on a simpler, pythonic version of the algorithm without the dancing links technique can be found here. . Using the python implementation of Algorithm X and converting our tile set to an exact cover bit matrix is left as an exercise for the reader :pencil2: . Pitfalls and Future Work . We have a decent set of algorithms that solve infinite loop puzzles. However, the image processing heuristics that we settled on are not the best. Some common faliure cases are, . When tile sizes change in the screenshot of the game, the bwToTiles function does not identify the correct tiles. | A similar problem occurs when the game is not centered to the screen. | Future work would be to extract game tiles from a screenshot with different tile sizes and non centered games. . Thank you for reading :grin: . Feel free to contact me at aliceoxenbury at gmail if you have any questions or comments. .",
            "url": "https://whoisankit.xyz/algorithms/2022/06/06/loop.html",
            "relUrl": "/algorithms/2022/06/06/loop.html",
            "date": " • Jun 6, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I’m Ankit Ramakrishnan and welcome to my blog. I’m a Computer Science Master’s student at Northeastern University interested in Computational Social Science, Digital Humanitites and Art. On this blog you will find my various projects and attempts at documenting my journey through academia. . This website is powered by fastpages 1. Unfortunately it’s way too over-powered for my needs. I might explore a different approach in the future. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://whoisankit.xyz/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://whoisankit.xyz/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}